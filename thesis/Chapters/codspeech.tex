\chapter{Codspeech}

\section{Project overview}


\section{Designing the DSL}


\section{Implementation}
This section will describe the various implementation steps taken
during the construction of Codspeech.


\subsection{PLY (Python Lex-Yacc)}\label{sec:ply}
PLY is an implementation of lex and yacc parsing tools, written purely
in python, by David Beazley \citep{code:online}. It was originally
developed for an intrdoctory class on compilers back in 2001. It
provides most of the standard lex/yacc features including support for
empty productions, precedence rules, error recovery, and support for
ambigous grammars. It uses LR-parsing, which is a reasonabe parsing
scheme for larger grammars, but slightly restrics the type of grammars
that can be written (\highlight{citation needed}). PLY is
straightforward to use, and one its many advantages is the \emph{very}
extensive error checking, which certainly makes life easier.

\subsection{Python Lex}
The first step to implement the language is to write a tokenizer. This
is done with the Lex module of PLY. Language tokens are recognized
using regular expressions, and the steps are straighforward.

The names of all the token types are declard as a list of strings
named \texttt{tokens}.

\lstset{
  caption = {The token list},
  label   = code:tokenlist
}
\begin{lstlisting}
class CodspeechLexer(object):

...

    tokens = [
        # Literals: identifier, type, integer constant, float
        # constant, string constant
        'IDENT', 'ICONST', 'FCONST', 'SCONST', 'DOCSTRING',

        # Assignments: = :
        'EQUALS', 'COLON',

        # Connection: <-
        'CONNECTION',

        # Delimeters: ( ) { } [ ] , .
        'LPAREN', 'RPAREN',
        'LBRACE', 'RBRACE',
        'LBRACKET', 'RBRACKET',
        'COMMA', 'PERIOD',

        # Other:
        'CR', 'OPTIONAL', 'OPTIONS'
    ]
\end{lstlisting}

Tokens that require no special processing are declared using
module-level variables prefixed by \texttt{t_}, where the name
following \texttt{t_} has to exactly match some string in the tokens
list. Each such variable contains a regular expression string that
matches the respective token (Python raw strings are usually used
since they are the most convenient way to write regular expression
strings).

\lstset{
  caption = {Token variables},
  label   = code:tokenvar
}
\begin{lstlisting}
class CodspeechLexer(object):

...

    t_EQUALS     = r'='
    t_COLON      = r':'
    t_CONNECTION = r'<-'
    t_LPAREN     = r'\('
    t_RPAREN     = r'\)'
    t_LBRACKET   = r'\['
    t_RBRACKET   = r'\]'
    t_LBRACE     = r'\{'
    t_RBRACE     = r'\}'
    t_COMMA      = r','
    t_PERIOD     = r'\.'
    t_OPTIONAL   = r'\?'
\end{lstlisting}

When tokens do require special processing, a token rule can be
specified as a function. For example, this rule matches numbers and
converts the string into a Python integer.

\lstset{
  caption = {Token functions},
  label   = code:tokenfunc 
}
\begin{lstlisting}
    def t_ICONST(self, t):
        r'\d+'
        t.value = int(t.value)
        return t
\end{lstlisting}

In some cases, we may want to build tokens from more complex regular
expressions. For example:

\lstset{
  caption = {Complex regular expressions},
  label   = code:regex
}
\begin{lstlisting}
class CodspeechLexer(object):

...

    lowercase    = r'[a-z]'
    identchar    = r'[_A-Za-z0-9-]'
    ident        = r'(' + lowercase + r'(' + identchar + r')*)'

    def t_IDENT(self, t):
        # we want the docstring to be the identifier above
        ...
\end{lstlisting}

\noindent This is not possible to specify using a normal docstring. The
programmer would have to write the full RE, defeating the purpose of
re-usable code. However, there is a way around this by using the
\texttt{@TOKEN} decorator.

\lstset{
  caption = {Token decoratior},
  label   = code:token
}
\begin{lstlisting}
from ply.lex import TOKEN

class CodspeechLexer(object):

...


    lowercase    = r'[a-z]'
    identchar    = r'[_A-Za-z0-9-]'
    ident        = r'(' + lowercase + r'(' + identchar + r')*)'

    @TOKEN(ident)
    def t_IDENT(self, t):
        t.type = self.keyword_map.get(t.value,"IDENT")
        return t
\end{lstlisting}

The observant reader might notice something special goind on in the
function \texttt{t_IDENT}. The processed string is checked against a
keyword map to decide wheter the token type should actually be
\texttt{IDENT} or something else. The keyword map is defined as a
dictionary, and the values are appended to the token list.

\lstset{
  caption = {Keyword map},
  label   = {code:keywordmap}
}
\begin{lstlisting}
class CodspeechLexer(object):

...

    keyword_map = {
        # Import
        'import'          : 'IMPORT',

        # Type
        'type'            : 'TYPE',

        # Atom keywords
        'atom'            : 'ATOM',
        #'options'         : 'OPTIONS',
        'python'          : 'ATOMTYPE',
        'python-extended' : 'ATOMTYPE',
        'external'        : 'ATOMTYPE',

        # Network
        'network'         : 'NETWORK',
        'controller'      : 'CONTROLLER',
                
        # Header
        'in'              : 'IN',
        'out'             : 'OUT',
        'default'         : 'DEFAULT',
        
        # Types
        'file'            : 'FILE',
        'float'           : 'FLOAT',
        'int'             : 'INT',
        'string'          : 'STRING',
    }


    tokens = [
        ...
    ] + list(set(keyword_map.values()))
\end{lstlisting}

\noindent Since our keyword map contains multiple keys mapping to the
same value and the token list can not contain any duplicates, the list
of values is converted to a set before it is converted back into a
list.


\subsection{Python Yacc}
The \texttt{yacc.py} module is used to parse the language
syntax. \emph{Syntax} is usually specified in terms of a
\emph{BNF-grammar} (\highlight{citation needed}). For example, some
simple grammar rules for parsing types could look like this:

\begin{figure}[h!]
  \begin{grammar}
    <type> ::= `float'
    \alt `int'
    \alt `string'
    \alt <type> <dim>

    <dim> ::= `[]'
    \alt <dim> `[]'
  \end{grammar}
  \caption{An example grammar for type identifiers}
  \label{grammar:typeex}
\end{figure}

\noindent The identifiers \emph{type} and \emph{dim} refer to grammar
rules comprised of a collection of \emph{terminals} and
\emph{non-terminals}. The symbols \texttt{float}, \texttt{int},
\texttt{string} and \texttt{[]} are known as the \emph{terminals}
and correspond to raw input tokens. The \emph{non-terminals}, such as
\emph{dim}, refer to other rules.

The \emph{semantic} behaviour of a language is often specified using
syntax directed translation. Each symbol in a given grammar rule has a
set of attributes associated with them along with an action. The
action describes what to do whenever a particular grammar rule is
recognized.

Yacc uses a parsing technique called LR-parsing, also known as
\emph{shift-reduce} parsing. It is a bottom up scheme that tries to
match a sequence of lexical objects against the right-hand-side of
various grammar rules. Whenever a matching right-hand-side is found,
the appropriate action code is triggered and the grammar symbols are
replaced by the grammar symbol on the left-hand-side.

Implementing a parser in Python Yacc is fairly straight forward. The
list of tokens from the lexer module is imported and a series of
functions describing the grammar productions are defined. From the
gramar in \autoref{grammar:typeex} the corresponding Python code
becomes:

\lstset{
  caption = {Parser example},
  label   = code:parser
}
\begin{lstlisting}
    def p_type(self, p):
        """
        type : FILE
             | FLOAT
             | INT
             | STRING
             | IDENT
             | type dim
        """
        if len(p) == 2:
            p[0] = csast.Type(p[1])
        else:
            p[1].type += p[2]
            p[0] = p[1]


    def p_dim(self, p):
        """
        dim : LBRACKET RBRACKET
            | LBRACKET RBRACKET dim
        """
        if len(p) == 3:
            p[0] = '[]'
        else:
            p[0] = '[]' + p[3]
\end{lstlisting}

Each function has a doc string that contains the appropriate
context-free grammar specification. This idea was actually borrowed
from the SPARK toolkit. \citep{spark:online}. A function takes an
argument, \emph{p}, that contains a sequence, starting at index 1, of
values matching the symbols in the corresponding rule. The value
\texttt{p[0]} is mapped to the left-hand-side rule, while the values
in \texttt{p[1..]} are mapped to the grammar symbold on the
right-hand-side. The statements in the function body implements the
semantic actions of the rule. In this case, we use the parser to to
build an abstract syntax tree. This is described in more detail in
\autoref{sec:ast}.


\subsection{Alternative specification of Lexer and Parser}
As seen in the above examples, both the lexer and parser are defined
from instances of their own classes. The easiest way, however, is to
specify them directly in their own modules. The PLY documentation
explains this quite well, complete with examples. \citep{code:online}


\subsection{Codspeech Backus-Naur Form}\label{sec:bnf}
\input{Chapters/bnf}


\subsection{Abstract Syntax Tree}\label{sec:ast}
The idea behind an abstract syntax tree (AST) is to represent the
abstract syntactic structure of the source code in tree form. Each
node in the tree represents some structure occuring in the source. The
AST provides a good structure for later compiler stages since it omits
details having to do with the source language, and only contains
information about the essential structure of the program.

The AST is implemented using node classes for important language
constructs. All these node classes extends an abstract base
class. Since Python is dynamically typed, the concept of interfaces
does not really exist. Interfaces, commonly referred to as
``protocols'', are implicit. Determining these interfaces is based on
implementation introspection. The abstract base class looks like this:

\lstset{
  caption = {An abstract base class for AST nodes},
  label   = code:abstractnode
}
\begin{lstlisting}
class Node(object):
    """ Abstract base class for AST nodes.
    """
    def children(self):
        """ A sequence of all children that are Nodes
        """
        pass

    def show(
        self,
        buf=sys.stdout,
        offset=0,
        attrnames=False,
        nodenames=False,
        showcoord=False,
        _my_node_name=None):
        lead = ' ' * offset
        if nodenames and _my_node_name is not None:
            buf.write(
                lead + self.__class__.__name__+ ' <' + _my_node_name + '>: ')
        else:
            buf.write(lead + self.__class__.__name__+ ': ')

        if self.attr_names:
            if attrnames:
                nvlist = [(n, getattr(self,n)) for n in self.attr_names]
                attrstr = ', '.join('%s=%s' % nv for nv in nvlist)
            else:
                vlist = [getattr(self, n) for n in self.attr_names]
                attrstr = ', '.join('%s' % v for v in vlist)
            buf.write(attrstr)

        if showcoord:
            buf.write(' (at %s)' % self.coord)
        buf.write('\n')

        for (child_name, child) in self.children():
            child.show(
                buf,
                offset=offset + 2,
                attrnames=attrnames,
                nodenames=nodenames,
                showcoord=showcoord,
                _my_node_name=child_name)
\end{lstlisting}

\noindent This base class also contains a pretty printing function,
\texttt{show()}, that prints the entire tree below a the node from
which it was invoked from.

An AST node can be specified in the following way:

\lstset{
  caption = {Example of an AST node},
  label = code:samplenode
}
\begin{lstlisting}
class Header(Node):
    def __init__(self, ident, doc, inputs, outputs, coord=None):
        self.ident = ident
        self.doc = doc
        self.inputs = inputs
        self.outputs = outputs
        self.coord = coord

    def children(self):
        nodelist = []
        if self.ident is not None:
            nodelist.append(("ident", self.ident))
        if self.doc is not None:
            nodelist.append(("doc", self.doc))
        for i, child in enumerate(self.inputs or []):
            nodelist.append(("inputs[%d]" % i, child))
        for i, child in enumerate(self.outputs or []):
            nodelist.append(("outputs[%d]" % i, child))
        return tuple(nodelist)

    attr_names = ()
\end{lstlisting}

Python also does not support multiple dispatch at the language
defition or syntactic level, nor does it support method
overloading. However, the visitor pattern can be implemented using
method introspection. Another base class for visiting nodes is
defined:

\lstset{
  caption = {The NodeVisitor class},
  label   = code:nodevisitor
}
\begin{lstlisting}
class NodeVisitor(object):
    def visit(self, node):
        """ Visit a node.
        """
        method = 'visit_' + node.__class__.__name__
        visitor = getattr(self, method, self.generic_visit)
        return visitor(node)

    def generic_visit(self, node):
        """ Called if no explicit visitor function exists for a
            node. Implements preorder visiting of the node.
        """
        for c_name, c in node.children():
            self.visit(c)
\end{lstlisting}

\lstset{
  caption = {An example use of the NodeVisitor},
  label   = code:visitorexample
}
\begin{lstlisting}
class ConstantVisitor(NodeVisitor):
    def __init__(self):
        self.values = []

    def visit_Constant(self, node):
        self.values.append(node.value)

...

cv = ConstantVisitor()
cv.visit(node)
\end{lstlisting}


\subsection{Typechecker}
\subsection{XML generation}
\subsection{emacs mode}


\section{Future work}
