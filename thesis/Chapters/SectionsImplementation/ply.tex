PLY is an implementation of \texttt{lex} and \texttt{yacc} parsing
tools, written purely in python, by \citet{ply:online}. It was
originally developed for an introductory class on compilers back in
2001. It provides most of the standard lex/yacc features including
support for empty productions, precedence rules, error recovery, and
support for ambigous grammars. It uses LR-parsing, which is a
reasonabe parsing scheme for larger grammars, but slightly restricts
the type of grammars that can be written \citep{aho:2007}. PLY is
straight-forward to use, and one its many advantages is
the \emph{very} extensive error checking, which certainly makes life
easier.

\paragraph{Python Lex}
The first step to implement the language is to write a tokenizer. This
is done with the Lex module of PLY. Language tokens are recognized
using regular expressions, and the steps are straighforward.

The names of all the token types are declard as a list of strings
named \texttt{tokens}.

\lstset{
  caption = {The token list},
  label   = code:tokenlist
}
\begin{lstlisting}
class CodspeechLexer(object):

...

    tokens = [
        # Literals: identifier, type, integer constant, float
        # constant, string constant
        'IDENT', 'ICONST', 'FCONST', 'SCONST', 'DOCSTRING',

        # Assignments: = :
        'EQUALS', 'COLON',

        # Connection: <-
        'CONNECTION',

        # Delimeters: ( ) { } [ ] , .
        'LPAREN', 'RPAREN',
        'LBRACE', 'RBRACE',
        'LBRACKET', 'RBRACKET',
        'COMMA', 'PERIOD',

        # Other:
        'CR', 'OPTIONAL', 'OPTIONS'
    ]
\end{lstlisting}

Tokens that require no special processing are declared using
module-level variables prefixed by \texttt{t_}, where the name
following \texttt{t_} has to exactly match some string in the tokens
list. Each such variable contains a regular expression string that
matches the respective token (Python raw strings are usually used
since they are the most convenient way to write regular expression
strings).

\lstset{
  caption = {Token variables},
  label   = code:tokenvar
}
\begin{lstlisting}
class CodspeechLexer(object):

...

    t_EQUALS     = r'='
    t_COLON      = r':'
    t_CONNECTION = r'<-'
    t_LPAREN     = r'\('
    t_RPAREN     = r'\)'
    t_LBRACKET   = r'\['
    t_RBRACKET   = r'\]'
    t_LBRACE     = r'\{'
    t_RBRACE     = r'\}'
    t_COMMA      = r','
    t_PERIOD     = r'\.'
    t_OPTIONAL   = r'\?'
\end{lstlisting}

When tokens do require special processing, a token rule can be
specified as a function. For example, this rule matches numbers and
converts the string into a Python integer.

\lstset{
  caption = {Token functions},
  label   = code:tokenfunc 
}
\begin{lstlisting}
    def t_ICONST(self, t):
        r'\d+'
        t.value = int(t.value)
        return t
\end{lstlisting}

In some cases, we may want to build tokens from more complex regular
expressions. For example:

\lstset{
  caption = {Complex regular expressions},
  label   = code:regex
}
\begin{lstlisting}
class CodspeechLexer(object):

...

    lowercase    = r'[a-z]'
    identchar    = r'[_A-Za-z0-9-]'
    ident        = r'(' + lowercase + r'(' + identchar + r')*)'

    def t_IDENT(self, t):
        # we want the docstring to be the identifier above
        ...
\end{lstlisting}

\noindent This is not possible to specify using a normal docstring. The
programmer would have to write the full RE, defeating the purpose of
re-usable code. However, there is a way around this by using the
\texttt{@TOKEN} decorator.

\lstset{
  caption = {Token decoratior},
  label   = code:token
}
\begin{lstlisting}
from ply.lex import TOKEN

class CodspeechLexer(object):

...


    lowercase    = r'[a-z]'
    identchar    = r'[_A-Za-z0-9-]'
    ident        = r'(' + lowercase + r'(' + identchar + r')*)'

    @TOKEN(ident)
    def t_IDENT(self, t):
        t.type = self.keyword_map.get(t.value,"IDENT")
        return t
\end{lstlisting}

The observant reader might notice something special goind on in the
function \texttt{t_IDENT}. The processed string is checked against a
keyword map to decide wheter the token type should actually be
\texttt{IDENT} or something else. The keyword map is defined as a
dictionary, and the values are appended to the token list.

\lstset{
  caption = {Keyword map},
  label   = {code:keywordmap}
}
\begin{lstlisting}
class CodspeechLexer(object):

...

    keyword_map = {
        # Import
        'import'          : 'IMPORT',

        # Type
        'type'            : 'TYPE',

        # Atom keywords
        'atom'            : 'ATOM',
        #'options'         : 'OPTIONS',
        'python'          : 'ATOMTYPE',
        'python-extended' : 'ATOMTYPE',
        'external'        : 'ATOMTYPE',

        # Network
        'network'         : 'NETWORK',
        'controller'      : 'CONTROLLER',
                
        # Header
        'in'              : 'IN',
        'out'             : 'OUT',
        'default'         : 'DEFAULT',
        
        # Types
        'file'            : 'FILE',
        'float'           : 'FLOAT',
        'int'             : 'INT',
        'string'          : 'STRING',
    }


    tokens = [
        ...
    ] + list(set(keyword_map.values()))
\end{lstlisting}

\noindent Since our keyword map contains multiple keys mapping to the
same value and the token list can not contain any duplicates, the list
of values is converted to a set before it is converted back into a
list.


\paragraph{Python Yacc}
The \texttt{yacc.py} module is used to parse the language
syntax. \emph{Syntax} is usually specified in terms of a
\emph{BNF-grammar} (\highlight{citation needed}). For example, some
simple grammar rules for parsing types could look like this:

\begin{figure}[h!]
  \begin{grammar}
    <type> ::= `float'
    \alt `int'
    \alt `string'
    \alt <type> <dim>

    <dim> ::= `[]'
    \alt <dim> `[]'
  \end{grammar}
  \caption{An example grammar for type identifiers}
  \label{grammar:typeex}
\end{figure}

\noindent The identifiers \emph{type} and \emph{dim} refer to grammar
rules comprised of a collection of \emph{terminals} and
\emph{non-terminals}. The symbols \texttt{float}, \texttt{int},
\texttt{string} and \texttt{[]} are known as the \emph{terminals}
and correspond to raw input tokens. The \emph{non-terminals}, such as
\emph{dim}, refer to other rules.

The \emph{semantic} behaviour of a language is often specified using
syntax directed translation. Each symbol in a given grammar rule has a
set of attributes associated with them along with an action. The
action describes what to do whenever a particular grammar rule is
recognized.

Yacc uses a parsing technique called LR-parsing, also known as
\emph{shift-reduce} parsing. It is a bottom up scheme that tries to
match a sequence of lexical objects against the right-hand-side of
various grammar rules. Whenever a matching right-hand-side is found,
the appropriate action code is triggered and the grammar symbols are
replaced by the grammar symbol on the left-hand-side.

Implementing a parser in Python Yacc is fairly straight forward. The
list of tokens from the lexer module is imported and a series of
functions describing the grammar productions are defined. From the
gramar in \autoref{grammar:typeex} the corresponding Python code
becomes:

\lstset{
  caption = {Parser example},
  label   = code:parser
}
\begin{lstlisting}
    def p_type(self, p):
        """
        type : FILE
             | FLOAT
             | INT
             | STRING
             | IDENT
             | type dim
        """
        if len(p) == 2:
            p[0] = csast.Type(p[1])
        else:
            p[1].type += p[2]
            p[0] = p[1]


    def p_dim(self, p):
        """
        dim : LBRACKET RBRACKET
            | LBRACKET RBRACKET dim
        """
        if len(p) == 3:
            p[0] = '[]'
        else:
            p[0] = '[]' + p[3]
\end{lstlisting}

Each function has a doc string that contains the appropriate
context-free grammar specification. This idea was actually borrowed
from the SPARK toolkit \citep{spark:online}. A function takes an
argument, \emph{p}, that contains a sequence, starting at index 1, of
values matching the symbols in the corresponding rule. The value
\texttt{p[0]} is mapped to the left-hand-side rule, while the values
in \texttt{p[1..]} are mapped to the grammar symbold on the
right-hand-side. The statements in the function body implements the
semantic actions of the rule. In this case, we use the parser to to
build an abstract syntax tree. This is described in more detail in
\autoref{sec:ast}.


\paragraph{Alternative specification of Lexer and Parser}
As seen in the above examples, both the lexer and parser are defined
from instances of their own classes. The easiest way, however, is to
specify them directly in their own modules. The PLY documentation
explains this quite well, complete with examples \citep{ply:online}.
